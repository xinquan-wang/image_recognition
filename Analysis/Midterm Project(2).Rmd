---
title: "Applied Data Science:  Midterm Project"
author: ""
date: ""
output:
  prettydoc::html_pretty:
  theme: cayman
highlight: github
---

```{r setup, include=FALSE}
set.seed(72)
knitr::opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55))
```

```{r libraries, echo = FALSE}
library(data.table)
library(DT)
library(class)
library(glmnet)
library(caret)
library(e1071)
library(psych)
library(rpart)
library(randomForest)
library(nnet)
library(R.filesets)
library(plyr)
```

```{r source_files}

```

```{r functions}
# define sampling function 
sampling = function(x,value){
  x[sample(x = 1:x[,.N], size = value, replace = FALSE),]
}


# create formula
create.formula <- function(outcome.name, input.names, input.patterns = NA,
                         all.data.names = NA, return.as = "character") {    
  variable.names.from.patterns <- c()
  if (!is.na(input.patterns[1]) & !is.na(all.data.names[1])) {        
    pattern <- paste(input.patterns, collapse = "|")        
    variable.names.from.patterns <- all.data.names[grep(pattern = pattern,
                                                        x = all.data.names)]    
  }    
  all.input.names <- unique(c(input.names, variable.names.from.patterns))    
  all.input.names <- all.input.names[all.input.names != outcome.name]
  if (!is.na(all.data.names[1])) {        
    all.input.names <- all.input.names[all.input.names %in% all.data.names]    
  }    
  input.names.delineated <- sprintf("`%s`", all.input.names)    
  the.formula <- sprintf("`%s` ~ %s", outcome.name, paste(input.names.delineated,
                                                          collapse =" + "))
  if (return.as == "formula") {
    return(as.formula(the.formula))    
  }
  if (return.as != "formula") {
    return(the.formula)    
  }
}

# creat x and y
create.x.and.y <-function(the.formula, data) {
  require(data.table)
  setDT(data)    
  x <- model.matrix(object = as.formula(the.formula), data = data)    
  y.name <- trimws(x = gsub(pattern = "`", replacement = "", 
                            x = strsplit(x = the.formula, split = "~")[[1]][1],fixed =TRUE))    
  y <- data[as.numeric(rownames(x)), get(y.name)]
  return(list(x = x, y = y))}

# define round function 
round.numerics <- function(x, digits){
  if(is.numeric(x)){
    x <- round(x = x, digits = digits)
  }
  return(x)
}

# define iteration function
iteration <- function(model.function){
  tab <- NULL
  for (i in 1:3){
    for (j in 1:iterations){
      size <- n.values[i]
      data.name <- paste0("dat_",size,"_",j)
      results <- model.function(size = size, data.name = data.name)
      tab <- as.data.table(rbind(tab,results))
    }
  }
  return(tab)
}

# define scoring function
scoring <- function(model.function){
  output <- get(sprintf("%s.%s",model.function,"tab"))
  
  A = output[,Sample.Size]/training[,.N]
  B = min(output[,the.time]/60 , 1)
  C = output[,inaccuracy]
  
  Points <- 0.25*A + 0.25 * B + 0.5 * C
  
  result <- data.table(Model = output[,Model],
                       `Sample Size` = output[,Sample.Size],
                       Data = output[,data.name],
                       A = A,
                       B = B,
                       C = C,
                       Points = Points)
  
  return(result)
}

# define scoring summary function
scoring.summary <- function(scoring.result){
  scoring.summary = scoring.result[,.(A=mean(A),B=mean(B),C=mean(C),Points=mean(Points)),keyby = c("Model","Sample Size")]
  return(scoring.summary)
}

# define reporting function
reporting <- function(datatable.name){
  datatable <- datatable.name[,lapply(X=.SD, FUN = "round.numerics",digits = digits)]
  return(datatable(datatable))
}

# define plurality voting function
vote <- function(input){
  if (length(unique(table(input)))!=1){
  most <- names(which.max(table(input))) 
  } else {
    most <- input[1]
  }
  return(most)
}
```

```{r constants}
n.values <- c(500,1000,2000)
iterations <- 3
digits <- 4
train.file = "MNIST-fashion training set-49.csv"
testing.file = "MNIST-fashion testing set-49.csv"
fashion.name <- "label"
```

```{r load_data}
setwd("C:/Users/huang/Downloads")
training <- fread(input = train.file, verbose = FALSE)
testing <- fread(input = testing.file, verbose = FALSE)
```

```{r clean_data}
#check if any bad apple in independent variables
independent_list <- names(training[,-1])
tr_check = training[,lapply(.SD, FUN = function(x)!is.numeric(x)), .SDcol = independent_list]; tr_check
te_check = testing[,lapply(.SD, FUN = function(x)!is.numeric(x)), .SDcol = independent_list]; te_check

#check missing value
sum(is.na(training))
sum(is.na(testing))

#check if there is any pixels out of [0,255]
sum(training[,-1]<0|training[,-1]>255)
sum(testing[,-1]<0|testing[,-1]>255)

#check whether there is any speical in the label name in train/test dataset
train_uniq <- unique(training[,1])
test_uniq <- unique(testing[,1])
sum(train_uniq != test_uniq)
```
Seen from the result, both training and testing datasets are clean:
* All the independent variables are numeric
* No missing value
* All the pixels are in the range of [0,255]
* All the labels in testing dataset are from training dataset. There is no any special label name

```{r generate_samples}
sample_datasets = c()
for (i in n.values){
  for (j in 1:iterations){
    sample = sampling(training,i)
    name = paste0("dat_",i,"_",j)
    assign(name,sample)
    sample_datasets = append(sample_datasets,name)
    }
  }
sample_datasets
data.table("Sample Size" = n.values,
           "First Random Sample" = sample_datasets[c(1,4,7)],
           "Second Random Sample" = sample_datasets[c(2,5,8)],
           "Third Random Sample" = sample_datasets[c(3,6,9)])
```
Since there are 600000 observations in total in training dataset, we decided to use around 500 to 2000 out of the overall training dataset as sample datasets by using sampling function that set before to sample from the rows of the overall training data randomly without replacement.

## Introduction

This project will implement 10 machine learning models to solve image recognition problem. We will also evaluate the models' performance in terms of model type, sample size and running time and further discuss their pros and cons.  
The dataset we used is a set of images for different types of apparel from Zalando's article, consisting of a training set of 60,000 examples and a test set of 10,000 examples. The original dataset divided each image into 784 (28 by 28) pixels. And in our project, we have condensed these data into 49 pixels (7 by 7) per image to simplify the computations. For each dataset, we have a column called *label* which indicates the type of the product and 49 columns for *pixels*, Denoted as *pixels1*, *pixels2*, ... ,*pixels49*, these *pixels* columns provide the measurement for the images in grayscale.  
To solve the challenge, we first generate 9 different samples with 3 different sizes and 3 iterations each. Fro each sample,
we appliy the following 10 models to generate the predictive classification results.  
1. Multinomial logistic regression  
2. K-Nearest Neighbors 
3. Classification Tree
4. Random Forest  
5. Ridge Regression  
6. Lasso Regression 
7. Support Vector Machines
8. Generalized Boosted Regression Models - gbm  
9. Generalized Boosted Regression Models - xgboost  
10. Ensemble model  
In order to evaluate their quality, we introduce a score function by balancing the sample size, running time and prediction accuracy. Based on these results, we make some comparisons between these models, idntifying the "best" model for our dataset.

### Model 1: Ridge Regression

Ridge Regression is used to analyze multiple regression data with multicollinearity. It is simple and can prevent overfitting by adding a L2 penalty term in the cost funciton. However, due to its trading variance for bias, the output from ridge regression is not unbiased. From the scoreboard, we can tell that as sample size increases, model scores increase as well which is not desired, since our goal is to minimize the value of *Points*. Let's take a further look at the *A*, *B*, *C* score respectively. *A* score is not very informative because it's a kind of "fixed" value for each sample. It is interesting to see that the values of proportion of the predictions on the testing set that are incorrectly classified don't vary much among these 9 datasets. But running time increases dramatically as the sample size grows which is responsible for the high variability of *Points*. This may imply that ridge regression may be highly time-consuming in large sample.

```{r code_model1_development, eval = FALSE}
input.names <- names(training[, -1])
formula.fashion <- create.formula(outcome.name = fashion.name, input.names = input.names)

# create an empty dataframe to contain predictions
pred.frm.ridge.regression <- NULL

# create a function for ridge regression model
model.ridge.regression <- function(data.name,size) {
  toc <- Sys.time()
  train.data <- get(data.name)
  x.y.train <- create.x.and.y(the.formula = formula.fashion,
                              data = train.data) 
  # fit ridge regression model
  mod <- cv.glmnet(x = x.y.train$x, y = x.y.train$y, family = "multinomial", 
                   alpha = 0)   
  x.y.test <- create.x.and.y(the.formula = formula.fashion,
                             data = testing)   
  # prediction
  pred <- predict(object = mod, newx = x.y.test$x, type ="class", s = mod$lambda.min)    
  tic <- Sys.time()
  the.time <- (tic - toc) %>% as.numeric(., units = "secs")
  inaccuracy <- mean(pred != testing$label)

  output <- data.frame(Model = "Ridge Regression",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.ridge.regression <<- cbind(as.character(pred),pred.frm.ridge.regression)
  
  return(output)
}


# Running the function for 3 different sample sizes and 3 iterations for each size
ridge.regression.tab = iteration(model.function = model.ridge.regression)
colnames(pred.frm.ridge.regression) <- sample_datasets

# Formatting the tables with initial results
ridge.regression <- scoring('ridge.regression')
ridge.regression.table <- reporting(ridge.regression)

# Formatting the tables with average results
ridge.regression.summary <- scoring.summary(ridge.regression)
ridge.regression.summary.table <- reporting(ridge.regression.summary)

# save the results of the model
saveRDS(ridge.regression.table,"ridge.regression.table")
saveRDS(ridge.regression.summary.table,"ridge.regression.summary.table")
```

```{r load_model1}
# load the results of the model
loadRDS("ridge.regression.table")
loadRDS("ridge.regression.summary.table")
```

### Model 2: K-Nearest Neighbors   

K-Nearest Neighbors (KNN) is a non-parametric method. It is an algorithm that is useful for making classifications/predictions when there are potential non-linear boundaries separating classes or values. It is commonly used for its easy of interpretation and implementation. In our classification setting, KNN calculates the distance between a test object and all training objects. The test point will be assigned to the class that is most common among its **k** nearest neighbors. We do a trick here. Since we already know there are 10 different types of product, so it is reasonable to pick **k** = 10 in the KNN model. Notice that KNN performs better in large sample and the accuracy gets higher as sample size grows. Although it works well and quick in our problem, the computaion speed tends to decrease very fast as dataset grows due to the computation and sorting of distances between the test point and every training points. And it does not learn anything from the training data and simply uses the training data itself for classification! The other drawbacks of KNN lies in that it's hard to determine the value of **k**. In our case, the value of **k** is easy to spot but for other settings, there's no rule of thumb. And the computaion speed tends to decrease very fast as dataset grows due to the computation of distances between the test point and every training points. In addition, KNN will fail if the sample is not balance.

```{r code_model2_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.knn <- NULL

# create a function for knn model
model.knn <- function(data.name,size){
  toc <- Sys.time()
  train.data <- get(data.name)
  pred <- knn(train = train.data[, -1], test = testing[, -1], cl = train.data$label, k = 5)
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "K-Nearest Neighbors",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.knn <<- cbind(as.character(pred),pred.frm.knn)
    
  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
knn.tab = iteration(model.function = model.knn)
colnames(pred.frm.knn) <- sample_datasets

# Formatting the tables with initial results
knn <- scoring('knn')
knn.table <- reporting(knn)

# Formatting the tables with average results
knn.summary <- scoring.summary(knn)
knn.summary.table <- reporting(knn.summary)

# save the results of the model
saveRDS(knn.table,"knn.table")
saveRDS(knn.summary.table,"knn.summary.table")
```

```{r load_model2}
# load the results of the model
loadRDS("knn.table")
loadRDS("knn.summary.table")
```

### Model 3:  Classification Tree

A classification tree is a tree that predicts the value of target variable based on input features, so it is very easy to interpret visually and it works well with decision boundaries parellel to the feature axis, since it mirrors human decision making more closely. But seen from the result, the accuracy(C) is not good because of overfitting issue in decision tree. However, since this model doesn't cost much time, the overall score is not that bad.

```{r code_model3_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.classification.tree <- NULL

# create a function for Classification Tree
model.classification.tree <- function (data.name,size){
  
  toc <- Sys.time()
# modeling steps
  trControl = trainControl(method="cv",number=10) #10-fold cross validation
  tuneGrid = expand.grid(.cp=seq(0,0.1,0.001))
  set.seed(100)
  trainCV = train(as.factor(label)~.,data=get(data.name),
                method="rpart", trControl=trControl,tuneGrid=tuneGrid)
# prediction steps
  treeCV = rpart(as.factor(label)~.,data=testing,
               method="class",
               control=rpart.control(cp=trainCV$bestTune))
  pred = predict(treeCV,newdata=testing,type="class")
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "Classification Tree",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.classification.tree <<- cbind(as.character(pred),pred.frm.classification.tree)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
classification.tree.tab = iteration(model.function = model.classification.tree)
colnames(pred.frm.classification.tree) <- sample_datasets

# Formatting the tables with initial results
classification.tree <- scoring('classification.tree')
classification.tree.table <- reporting(classification.tree)

# Formatting the tables with average results
classification.tree.summary <- scoring.summary(classification.tree)
classification.tree.summary.table <- reporting(classification.tree.summary)

# save the results of the model
saveRDS(classification.tree.table,"classification.tree.table")
saveRDS(classification.tree.summary.table,"classification.tree.summary.table")
```

```{r load_model3}
# load the results of the model
loadRDS("classification.tree.table")
loadRDS("classification.tree.summary.table")
```

### Model 4: Support Vector Machines

Support Vector Machine is a supervised, non-linear, non-parametric classification technique. It constructs a hyperplane in an N-dimensional space which maximizes the distance to the nearest data points of any classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence. SVM does well on both structured and unstructured data. By generalizing data, SVM seems to have lower risk suffering from overfitting. Also, it delivers a unique solution, since the optimality problem is convex. The effectiveness of SVM is supported by our results. It does good job in terms of *Points* which are generally lower than other methods. Although it's an elegant and powerful algorithm, the results are difficult to understand and interpret. And the long training time for large datasets can not be ignored either.

```{r code_model4_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.svm <- NULL

# svm model
model.svm <- function(data.name,size){
  toc <- Sys.time()
  train.data <- get(data.name)
  # fit svm model
  mod <- svm(formula = as.formula(formula.fashion), data = train.data, type = "C")
  # prediction
  pred <- predict(object = mod, newdata = testing)
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "Support Vector Machines",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.svm <<- cbind(as.character(pred),pred.frm.svm)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
svm.tab = iteration(model.function = model.svm)
colnames(pred.frm.svm) <- sample_datasets

# Formatting the tables with initial results
svm <- scoring('svm')
svm.table <- reporting(svm)

# Formatting the tables with average results
svm.summary <- scoring.summary(svm)
svm.summary.table <- reporting(svm.summary)

# save the results of the model
saveRDS(svm.table,"svm.table")
saveRDS(svm.summary.table,"svm.summary.table")
```

```{r load_model4}
# load the results of the model
loadRDS("svm.table")
loadRDS("svm.summary.table")
```

### Model 5: Lesso Regression


```{r code_model5_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.lassowcv <- NULL

##Lasso with cross validation
model.lassowcv <- function(data.name,size){
  toc <- Sys.time()
  train.data <- get(data.name)
  #Does k-fold cross-validation for glmnet, produce best lambda
  cv <- cv.glmnet(as.matrix(train.data[,-1]),as.matrix(train.data[,1]), alpha=1,nfolds=5, family= "multinomial")
  bestlambda <- cv$lambda.min
  
  ##fitting Lasso Regression
  lasso.rg <- glmnet(as.matrix(train.data[,-1]),as.matrix(train.data[,1]), alpha = 1, lambda = bestlambda, family = "multinomial", type.multinomial = "grouped")  ##group
  ##predict classification
  pred <- predict(lasso.rg, newx = as.matrix(testing[,-1]), type = "class")  
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "Lasso with cross validation",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.lassowcv <<- cbind(as.character(pred),pred.frm.lassowcv)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
lassowcv.tab = iteration(model.function = model.lassowcv)
colnames(pred.frm.lassowcv) <- sample_datasets

# Formatting the tables with initial results
lassowcv <- scoring('lassowcv')
lassowcv.table <- reporting(lassowcv)

# Formatting the tables with average results
lassowcv.summary <- scoring.summary(lassowcv)
lassowcv.summary.table <- reporting(lassowcv.summary)

# save the results of the model
saveRDS(lassowcv.table,"lassowcv.table")
saveRDS(lassowcv.summary.table,"lassowcv.summary.table")

# create an empty dataframe to contain predictions
pred.frm.lasso <- NULL

##Lasso without cross validation
model.lasso <- function(data.name,size){
  toc <- Sys.time()
  train.data <- get(data.name)
  ##fitting Lasso Regression
  lasso.rg2 <- glmnet(as.matrix(train.data[,-1]),as.matrix(train.data[,1]), alpha = 1, family = "multinomial")  ##group
  ##predict classification
  pred <- predict(lasso.rg2, newx = as.matrix(testing[,-1]), type = "class") 
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "Lasso without cross validation",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.lasso <<- cbind(as.character(pred),pred.frm.lasso)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
lasso.tab = iteration(model.function = model.lasso)
colnames(pred.frm.lasso) <- sample_datasets

# Formatting the tables with initial results
lasso <- scoring('lasso')
lasso.table <- reporting(lasso)

# Formatting the tables with average results
lasso.summary <- scoring.summary(lasso)
lasso.summary.table <- reporting(lasso.summary)

# save the results of the model
saveRDS(lasso.table,"lasso.table")
saveRDS(lasso.summary.table,"lasso.summary.table")
```

```{r load_model5}
# load the results of the model
loadRDS("lassowcv.table")
loadRDS("lassowcv.summary.table")
loadRDS("lasso.table")
loadRDS("lasso.summary.table")
```


### Model 6: Multinomial logistic regression 


```{r code_model6_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.multinomial.LR <- NULL

# create a function for multinomial logistic regression
model.multinomial.LR <- function(data.name,size){
  toc <- Sys.time()
  train.data <- get(data.name)
  the.formula. <- create.formula(outcome.name = fashion.name, input.names = input.names)
  
  ##fitting multinomial logistic regression model
  multilr <- multinom(as.formula(the.formula.),train.data)
  ##predict classification
  pred <- predict(multilr, testing[,-1])
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "Multinomial logistic regression",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.multinomial.LR <<- cbind(as.character(pred),pred.frm.multinomial.LR)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
LR.tab = iteration(model.function = model.multinomial.LR)
colnames(pred.frm.multinomial.LR) <- sample_datasets

# Formatting the tables with initial results
LR <- scoring('LR')
LR.table <- reporting(LR)

# Formatting the tables with average results
LR.summary <- scoring.summary(LR)
LR.summary.table <- reporting(LR.summary)

# save the results of the model
saveRDS(LR.table,"LR.table")
saveRDS(LR.summary.table,"LR.summary.table")
```

```{r load_model6}
# load the results of the model
loadRDS("LR.table")
loadRDS("LR.summary.table")
```

### Model 7: Random Forest 


```{r code_model7_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.RF <- NULL

# create a function for Random Forest
model.RF <- function(data.name,size){
  toc <- Sys.time()
  train.data <- get(data.name)
  train.data[,label := as.factor(label)]
  ##fitting multinomial logistic regression model
  rf<-randomForest(as.formula(formula.fashion), train.data, mtry=sqrt(49), importance = T, proximity = T)
  ##predict classification
  pred<-predict(rf, testing[,-1])
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "Random Forest",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.RF <<- cbind(as.character(pred),pred.frm.RF)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
RF.tab = iteration(model.function = model.RF)
colnames(pred.frm.RF) <- sample_datasets

# Formatting the tables with initial results
RF <- scoring('RF')
RF.table <- reporting(RF)

# Formatting the tables with average results
RF.summary <- scoring.summary(RF)
RF.summary.table <- reporting(RF.summary)

# save the results of the model
saveRDS(RF.table,"RF.table")
saveRDS(RF.summary.table,"RF.summary.table")
```

```{r load_model7}
# load the results of the model
loadRDS("RF.table")
loadRDS("RF.summary.table")
```

### Model 8: Generalized Boosted Regression Models - gbm

Gradient Boosted Methods generally have 3 parameters to train: shrinkage parameter, depth of tree, number of trees. Each of these parameters should be tuned to get a good fit. GBMs build an ensemble of weak and shallow successive trees with each tree learning and improving from the previous one, by using the same or slightly different in parameter setting's base learners to adaptively fit the data. The greatest advantage of GBMs is that it doesn't require any data pre-processing and can provide predictive accuracy that cannot be beat by other models with lots of flexibility. However, in this case, since we take running time into account, we didn't tune many parameters in the model and the accuracy is as good as expected. This reveals the disadvantage of GBMs is that GBMs often require many trees which can be highly time and memory exhaustive, since trees are built iteratively.

```{r code_model8_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.gbm <- NULL

#create function for gbm
model.gbm <- function (data.name,size){
  set.seed(1)
  
  toc <- Sys.time()
  # modeling steps
  # Fit the model on the training set
  set.seed(123)
  trControl=trainControl(method = "cv",
                         number = 5)
  tuneGrid=  expand.grid(n.trees = 300,
                         interaction.depth = 2,
                         shrinkage = 0.15,
                         n.minobsinnode=10)
  cvBoost = train(as.factor(label)~.,
                  data=get(data.name),
                  method="gbm",
                  trControl=trControl,
                  tuneGrid=tuneGrid)
  # Make predictions on the test data
  pred <- predict(cvBoost, newdata = testing, n.trees = 300,type="raw")
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "gbm",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.gbm <<- cbind(as.character(pred),pred.frm.gbm)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
gbm.tab = iteration(model.function = model.gbm)
colnames(pred.frm.gbm) <- sample_datasets

# Formatting the tables with initial results
gbm <- scoring('gbm')
gbm.table <- reporting(gbm)

# Formatting the tables with average results
gbm.summary <- scoring.summary(gbm)
gbm.summary.table <- reporting(gbm.summary)

# save the results of the model
saveRDS(gbm.table,"gbm.table")
saveRDS(gbm.summary.table,"gbm.summary.table")
```

```{r load_model8}
# load the results of the model
loadRDS("gbm.table")
loadRDS("gbm.summary.table")
```

### Model 9: Generalized Boosted Regression Models - xgboost

XGBoost Algorithm is a scalable and accurate implementation of gradient boosted decision trees, which was designed for speed and performance. As has been mentioned above, boosting is an ensemble method that seeks to create a stronger classifier (model) based on previous "weak" classifiers, which means this algorithm is trying to build a stronger correlation between the learners and the actual target variable. By adding models on top of each other iteratively, the errors of the previous model will be corrected by the next predictor, until the training data is accurately predicted or reproduced by the model. 

In this case, the overall result is not that good, even compared to other simplier models, because of time consuming(B) and inaccuracy rate(C).

```{r code_model9_development, eval = FALSE}
# create an empty dataframe to contain predictions
pred.frm.xgboost <- NULL
#create function for xgboost
model.xgboost <- function (data.name,size){
  set.seed(1)
  
  toc <- Sys.time()
  # modeling steps
  # Fit the model on the training set
  xgbTree <- train(as.factor(label)~., 
               data = get(data.name), 
               method = "xgbTree",
               trControl = trainControl("cv", number = 5)
               )
  # Make predictions on the test data
  pred <- predict(xgbTree, testing, type="raw")
  tic <- Sys.time()
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  output <- data.frame(Model = "xgboost",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  pred.frm.xgboost <<- cbind(as.character(pred),pred.frm.xgboost)

  return(output)
}

# Running the function for 3 different sample sizes and 3 iterations for each size
xgboost.tab = iteration(model.function = model.xgboost)
colnames(pred.frm.xgboost) <- sample_datasets

# Formatting the tables with initial results
xgboost <- scoring('xgboost')
xgboost.table <- reporting(xgboost)

# Formatting the tables with average results
xgboost.summary <- scoring.summary(xgboost)
xgboost.summary.table <- reporting(xgboost.summary)

# save the results of the model
saveRDS(xgboost.table,"xgboost.table")
saveRDS(xgboost.summary.table,"xgboost.summary.table")
```

```{r load_model9}
# load the results of the model
loadRDS("xgboost.table")
loadRDS("xgboost.summary.table")
```

### Model 10: Ensemble model 


```{r code_model10_development, eval = FALSE}
# create a function for ensemble model
model.ensemble <- function(data.name,size){
  
  pred.mat <- cbind(pred.frm.multinomial.LR[,data.name],pred.frm.knn[,data.name],pred.frm.ridge.regression[,data.name],pred.frm.classification.tree[,data.name])
  # Setting starting time
  toc <- Sys.time()
  
  # Make predictions on the test data
  pred <- apply(pred.mat,1,vote)
  # Ending time
  tic <- Sys.time()
  
  the.time <- as.numeric(x = tic-toc, units = "secs")
  inaccuracy <- mean(pred != testing$label)
  
  # Model summary
  output <- data.frame(Model = "Ensemble model",
                       Sample.Size = size,
                       data.name = data.name,
                       the.time = the.time,
                       inaccuracy = inaccuracy)
  return(output)                   
}

# Running the function for 3 different sample sizes and 3 iterations for each size
ensemble.tab = iteration(model.function = model.ensemble)


# Formatting the tables with initial results
ensemble <- scoring('ensemble')
ensemble.table <- reporting(ensemble)

# Formatting the tables with average results
ensemble.summary <- scoring.summary(ensemble)
ensemble.summary.table <- reporting(ensemble.summary)

# save the results of the model
saveRDS(ensemble.table,"ensemble.table")
saveRDS(ensemble.summary.table,"ensemble.summary.table")
```

```{r load_model10}
# load the results of the model
loadRDS("ensemble.table")
loadRDS("ensemble.summary.table")
```

## Scoreboard

```{r scoreboard, eval = FALSE}
scoreboard <- setorderv(as.data.table(rbind(loadRDS("ridge.regression.table"),loadRDS("knn.table"),loadRDS("classification.tree.table"),loadRDS("svm.table"),loadRDS("LR.table"),loadRDS("RF.table"),loadRDS("gbm.table"),loadRDS("xgboost.table"),loadRDS("lasso.table"))), "Points",1)
datatable(scoreboard)

scoreboard.average <- setorderv(as.data.table(rbind(loadRDS("ridge.regression.summary.table"),loadRDS("knn.summary.table"),loadRDS("classification.tree.summary.table"),loadRDS("svm.summary.table"),loadRDS("LR.summary.table"),loadRDS("RF.summary.table"),loadRDS("gbm.summary.table"),loadRDS("xgboost.summary.table"),loadRDS("lasso.summary.table"))), "Points",1)
datatable(scoreboard.average)
```

## Discussion

Seen from the consolidated table, among all these datatables 

Changing the weights of sample size component *A* the running time factor *B* or the accuracy *C* will definitely affect the model evaluation. If we give more weight on *A*, the value of *Points* will be dominated by the sample size. Therefore, different models with the same sample size may have similar scores. As for attaching more weight on *B*, then we are trying to stress more attention on the running time. The model with least running time will win. So it is possible that the models with sample size of 500 are potential winners, since in general the smaller the sample, the less the running time.  

If given the computing resources to explore a wider variety of models and sample sizes, we would choose the sample size with different scales to get more distinct dataset. And maybe we can filter the models that are not appriopiate for large dataset out before modeling large data.

## References
Create.formula function from Gr5243's lecture five.

